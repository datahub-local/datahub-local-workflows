<context>
# Overview  

This is a personal data project to learn / use a new data stack which help me to improve my personal career in the Data industry.

It will consist of mini-projects that will cover the data lycycle:
- Data generation from API or public datasets.
- Data ingestion to temporal stores using a ETL framework.
- Creation of data model in a Datalakehouse.
- Data exploration using BI tools.

# Core Features  

- Data Ingestion
- Data Model generation in a DW
- Data exploration
</context>
<PRD>
# Technical Architecture  

Architecture (components):
- Orchestration: Apache Airflow (KubernetesPodOperator for containerized DLT/dbt runs).
- Runtime: Kubernetes cluster running Airflow, Trino, MinIO, Kafka (optional), and supporting services.
- Object storage: MinIO (S3 API) for Iceberg data and raw artifacts.
- Table format & catalog: Apache Iceberg, Catalog: Nessie backed by Postgres (recommended).
- Query engine: Trino (Iceberg connector) â€” Superset connects to Trino.
- Ingestion:
  - Batch: API-by-date or S3 file ingestion.
  - Streaming: Kafka (or n8n -> Kafka/HTTP); DLT container (Spark or Python consumer) writes Iceberg partitions to MinIO.
- Transformations: dbt (containerized) produces Silver/Gold models; materialize via Trino or a write-capable engine.
- Metadata & state: Postgres for Airflow metadata and Hive/Metastore needs; container registry for images.
- Observability: Prometheus + Grafana; Loki for logs.
- Secrets: Kubernetes secrets (env vars) for credentials.

Data model & ingestion pattern (medallion)
- Bronze (raw events)
    - Iceberg tables. Examples: bronze.invoices_raw
    - Schema: raw JSON columns + ingestion metadata (offset, kafka_topic, ingested_at)
    - Partitioning: tipically by date (ingest_date).
- Silver (cleaned, canonical)
    - Iceberg tables. Examples: silver.invoices, silver.products, silver.shops.
    - Dedup keys and canonical identifiers. Examples: product_id, shop_id.
    - Partitioning: id or date.
- Gold (analytics marts)
    - Iceberg tables. Examples: marts.invoice_summary, marts.product_price_history
    - Aggregated, optimized for Trino queries and Superset dashboards

Superset integration
- Superset connects to Trino via SQLAlchemy Trino connector.
- Dashboards query gold marts.

Infrastructure requirements & sizing
- Kubernetes cluster with:
    - Control plane and worker nodes sized for Spark/dbt peak loads.
    - Storage: MinIO cluster with enough capacity and erasure coding for durability.
    - Databases: Postgres (Airflow + Hive Metastore); consider HA.
- CI/CD
    - Build and push DLT and dbt images on changes.
    - Run DLT / dbt tests in CI;
    - DAGs are deployed automatically by Airflow.

Operational notes & best practices
- Use Airflow variables & connections to centralize credentials.
- Create idempotent DLT/dbt jobs
- Keep Iceberg metadata small via compaction; tune manifest size.
- Enable schema evolution policies and automated tests in dbt to prevent drift.

# Development Roadmap  

- Initial Project setup for every technology.
- Projects A - PantryPulse (Grocery Invoice Ingestor):
    - Data will come calling an n8n API that will generate data of invoices.
    - Generate data model with:
        - Invoice
        - Product
        - Shops (brand, locations)
        - Etc.
    - Create some dashboard that will show:
        - Invoice list and Invoice details.
        - Product detail and price over time.
        - Monthly/Yearly expenses grouped by shop / shop brands / product type / product brand
    - Other requirements:
        - Maintain data consistency: so there are not multiple products with same name, multiple shops.
        - Streaming data will be received by kafka: so there are not multiple products with same name, multiple shops.

# Logical Dependency Chain

1) Create ingestion job: fetch or consume source data -> landing in `bronze` Iceberg table (DLT/Spark or Python consumer).
2) Create dbt models: transform `bronze` -> `silver` (cleaning, dedup) and `silver` -> `gold` marts.
3) Add Airflow DAGs: orchestrate ingestion and dbt runs with clear dependencies and retries.
4) Create Superset datasets/dashboards: connect Superset to Trino and build visualizations.

</PRD>